{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e020808-b006-49fb-83b7-e1edade69320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e65dc9-a24f-4b38-9524-3029d723f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15293dd-3218-4a38-8430-4da992c96fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 128)               24704     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,994\n",
      "Trainable params: 89,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 07:40:54.172766: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# the output of SimpleRNN will be a 2d tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(128))\n",
    "# add an additional hidden layer\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# view architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225a3ee4-45b6-46bf-a4b2-7579b47237f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               98816     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM network example\n",
    "model = keras.Sequential()\n",
    "# add an embedding layer\n",
    "model.add(layers.Embedding(input_dim=1000,output_dim=64))\n",
    "# add a LSTM layer\n",
    "model.add(layers.LSTM(128))\n",
    "# add a dense layer\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7dac61-73da-43b6-9e99-b00794921132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "url = \"https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-practice-datasets/main/unit_4/sherlock.txt\"\n",
    "res = requests.get(url)\n",
    "text = res.text\n",
    "text = text.replace('\\r\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a630e7-1bc7-4bef-b000-fa3b054bcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the text: 91\n"
     ]
    }
   ],
   "source": [
    "# encode data as chars\n",
    "# find unique chars\n",
    "chars = list(set(text))\n",
    "# lookup tables\n",
    "char_int = {c:i for i, c in enumerate(chars)}\n",
    "int_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "print('The number of unique characters in the text:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38edca91-da15-4384-bda3-9b31fde74591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 54974\n"
     ]
    }
   ],
   "source": [
    "# create the sequence data\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "# encode the characters using the lookup tables\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "# init empty lists to hold the sequences\n",
    "sequences = [] # each element is 40 chars long\n",
    "next_char = [] # one element for each sequence\n",
    "\n",
    "# loop through the entire text\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i:i+maxlen])\n",
    "    next_char.append(encoded[i+maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedd610a-08ae-41e2-898b-88e36d8569e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences so all are equal\n",
    "seq = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=40)\n",
    "# create x & y, create arrays of zeros (False)\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "# turn on the location (set to True) when the character is present\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "    y[i,next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6a7468-4830-4556-877c-9917c851b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=64, input_dim=len(chars)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6635db-766f-4c27-a986-b4cd4ef31d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1718/1718 - 21s - loss: 2.1951 - 21s/epoch - 12ms/step\n",
      "Epoch 2/5\n",
      "1718/1718 - 23s - loss: 2.0643 - 23s/epoch - 14ms/step\n",
      "Epoch 3/5\n",
      "1718/1718 - 24s - loss: 1.9871 - 24s/epoch - 14ms/step\n",
      "Epoch 4/5\n",
      "1718/1718 - 24s - loss: 1.9271 - 24s/epoch - 14ms/step\n",
      "Epoch 5/5\n",
      "1718/1718 - 23s - loss: 1.8757 - 23s/epoch - 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcad036b820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(seq, y, batch_size=32, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a0f1cc-0b71-438e-89cd-ae5efb708e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, length):\n",
    "    encoded = [char_int[c] for c in seed]\n",
    "    generated = ''\n",
    "    generated += seed\n",
    "    model.reset_states()\n",
    "    start_index = 0\n",
    "    \n",
    "    for _ in range(length):\n",
    "        sample = encoded[start_index:start_index+10]\n",
    "        sample = np.array(sample)\n",
    "        sample = np.expand_dims(sample, 0)\n",
    "        \n",
    "        pred = model.predict(sample)\n",
    "        pred = tf.squeeze(pred, 0)\n",
    "        next_char = np.argmax(pred)\n",
    "        encoded.append(next_char)\n",
    "        generated += int_char[next_char]\n",
    "        \n",
    "        start_index += 1\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32fdec92-6247-4f4c-b97e-00754e2b4874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theoriesmothrtoa hn tn t sonenetltant re th the r ne te ore tf  had hothrtn trt nee af  ae hng ah thes  tarh  ah ttptethe r nd er e  rttaoahothohu     e r  aeatheahe coooathrtf  thotmav tau e  etheeao  t  tm dhaoamneahe  tmhte tm eahoear e aeooohnttvsmeoan     er rommmmvtetmsdd e r  aoumnn e  atmheu enehhtttvtmvhernldtmhtmheIeetdde rr damheeerthehnre dd tvhsdnnn    eeernr tmmmmveteueeeeeor eteetvteoms    '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed text which model will use to generate predicted text\n",
    "seed_text = \"I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theories\"\n",
    "\n",
    "generate_text(model, seed_text, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "661dff71-b1e8-4262-942c-1722e1dcd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1718/1718 - 24s - loss: 1.8326 - 24s/epoch - 14ms/step\n",
      "Epoch 2/10\n",
      "1718/1718 - 24s - loss: 1.7923 - 24s/epoch - 14ms/step\n",
      "Epoch 3/10\n",
      "1718/1718 - 24s - loss: 1.7558 - 24s/epoch - 14ms/step\n",
      "Epoch 4/10\n",
      "1718/1718 - 24s - loss: 1.7192 - 24s/epoch - 14ms/step\n",
      "Epoch 5/10\n",
      "1718/1718 - 25s - loss: 1.6856 - 25s/epoch - 14ms/step\n",
      "Epoch 6/10\n",
      "1718/1718 - 23s - loss: 1.6532 - 23s/epoch - 13ms/step\n",
      "Epoch 7/10\n",
      "1718/1718 - 24s - loss: 1.6204 - 24s/epoch - 14ms/step\n",
      "Epoch 8/10\n",
      "1718/1718 - 25s - loss: 1.5898 - 25s/epoch - 14ms/step\n",
      "Epoch 9/10\n",
      "1718/1718 - 24s - loss: 1.5595 - 24s/epoch - 14ms/step\n",
      "Epoch 10/10\n",
      "1718/1718 - 25s - loss: 1.5311 - 25s/epoch - 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcae07b8a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train with more epochs\n",
    "model.fit(seq, y, batch_size=32, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e761fb9a-4c77-402e-a8ba-f3e6c69350ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theoriesborhiwoa tt ws tmdaspssrltant ne hh she smnedoee r  anl miv boytiws tr  nle afe ae hng ah this htattieth tuttethenddnd eueonirrirwCa the   e eea rg oowwaaaao caaan usdoeaImdymaneoei sna ahattyy t ost dtaoamn ahes saorhecdh ahr en  r     hvgdt nsrgggarpherlI dbvnrceotndn rrrrdhrmgrdIs ewst’   an  t natetlrrde   bho t aaant e t    ahi derte an aavgt emm mmaeaeaooee eldeeer o soue eomn e  oeieeaaaaee'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, seed_text, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4202f-3fc6-41f2-b756-e21a34d766f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
